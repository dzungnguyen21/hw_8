{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Constants\n",
    "PLAYER_X = 1\n",
    "PLAYER_O = -1\n",
    "EMPTY = 0\n",
    "ALPHA = 0.1     # Learning rate\n",
    "GAMMA = 0.9     # Discount factor\n",
    "EPSILON = 0.1   # Exploration rate\n",
    "BOARD_SIZE_4x4 = 4\n",
    "BOARD_SIZE_5x5 = 5\n",
    "  # Board size for Tic Tac Toe (3x3)\n",
    "REPLAY_BUFFER_SIZE = 2000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create an empty board\n",
    "def create_board(size):\n",
    "    return np.zeros((size, size), dtype=int)\n",
    "\n",
    "# Check for a winner\n",
    "def check_winner(board):\n",
    "    size = board.shape[0]\n",
    "    for player in [PLAYER_X, PLAYER_O]:\n",
    "        for i in range(size):\n",
    "            if all(board[i, :] == player) or all(board[:, i] == player):\n",
    "                return player\n",
    "        if all([board[i, i] == player for i in range(size)]) or all([board[i, size - 1 - i] == player for i in range(size)]):\n",
    "            return player\n",
    "    if np.all(board != EMPTY):\n",
    "        return 0  # Draw\n",
    "    return None  # Game not finished\n",
    "\n",
    "class DeepQLearningAgent:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        \n",
    "        self.replay_buffer = deque(maxlen=REPLAY_BUFFER_SIZE)\n",
    "        self.epsilon = EPSILON\n",
    "    \n",
    "    def build_model(self):\n",
    "        \"\"\"Build the neural network for Q-value approximation.\"\"\"\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(self.size, self.size)),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.size * self.size)  # Output Q-value for each cell in the board\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=ALPHA), loss='mse')\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Copy weights from the main model to the target model.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def get_possible_actions(self, board):\n",
    "        \"\"\"Get a list of possible actions (empty cells).\"\"\"\n",
    "        return list(zip(*np.where(board == EMPTY)))\n",
    "\n",
    "    def choose_action(self, board):\n",
    "        \"\"\"Epsilon-greedy policy to choose the next action.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose a random action\n",
    "            return random.choice(self.get_possible_actions(board))\n",
    "        \n",
    "        # Exploit: choose the best action based on Q-values\n",
    "        q_values = self.model.predict(board.reshape(1, self.size, self.size), verbose=0)\n",
    "        q_values = q_values.reshape(self.size, self.size)\n",
    "        \n",
    "        # Mask invalid actions (filled cells)\n",
    "        possible_actions = self.get_possible_actions(board)\n",
    "        best_action = max(possible_actions, key=lambda x: q_values[x[0], x[1]])\n",
    "        return best_action\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in the replay buffer.\"\"\"\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"Train the model using experiences sampled from the replay buffer.\"\"\"\n",
    "        if len(self.replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        minibatch = random.sample(self.replay_buffer, BATCH_SIZE)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                next_q_values = self.target_model.predict(next_state.reshape(1, self.size, self.size), verbose=0)\n",
    "                target = reward + GAMMA * np.max(next_q_values)\n",
    "            \n",
    "            # Update the Q-value for the chosen action\n",
    "            q_values = self.model.predict(state.reshape(1, self.size, self.size), verbose=0)\n",
    "            q_values[0][action[0] * self.size + action[1]] = target\n",
    "            \n",
    "            # Train the model\n",
    "            self.model.fit(state.reshape(1, self.size, self.size), q_values, epochs=1, verbose=0)\n",
    "\n",
    "    def play_game(self, opponent='random'):\n",
    "        \"\"\"Simulate a game between the DQL agent and a random opponent.\"\"\"\n",
    "        board = create_board(self.size)\n",
    "        player = PLAYER_X\n",
    "        state = board.copy()\n",
    "\n",
    "        while True:\n",
    "            if player == PLAYER_X:\n",
    "                action = self.choose_action(board)\n",
    "            else:\n",
    "                action = random.choice(self.get_possible_actions(board)) if opponent == 'random' else self.choose_action(board)\n",
    "\n",
    "            board[action] = player\n",
    "            next_state = board.copy()\n",
    "            winner = check_winner(board)\n",
    "            \n",
    "            reward = 1 if winner == PLAYER_X else -1 if winner == PLAYER_O else 0\n",
    "            done = winner is not None\n",
    "            \n",
    "            self.remember(state, action, reward, next_state, done)\n",
    "\n",
    "            if done:\n",
    "                return reward\n",
    "            \n",
    "            if player == PLAYER_X:\n",
    "                self.replay()\n",
    "\n",
    "            state = next_state\n",
    "            player = -player\n",
    "\n",
    "    def train(self, episodes, target_update_interval=10):\n",
    "        win_count = 0\n",
    "        win_rates = []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            if self.play_game() == 1:\n",
    "                win_count += 1\n",
    "\n",
    "            if (episode + 1) % target_update_interval == 0:\n",
    "                self.update_target_model()\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                win_rate = win_count / (episode + 1)\n",
    "                win_rates.append(win_rate)\n",
    "                print(f\"Episode {episode + 1}: Win rate = {win_rate:.3f}\")\n",
    "\n",
    "        return win_rates\n",
    "\n",
    "def plot_win_rate(win_rates, board_size):\n",
    "    plt.plot(win_rates)\n",
    "    plt.xlabel('Episodes (x100)')\n",
    "    plt.ylabel('Win Rate')\n",
    "    plt.title(f'Agent Win Rate Over Time (Board Size: {board_size}x{board_size})')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    agent_4x4 = DeepQLearningAgent(BOARD_SIZE_4x4)\n",
    "    win_rate_4x4 = agent_4x4.train(1000)\n",
    "    \n",
    "    # Plotting win rate\\\n",
    "    plot_win_rate(win_rate_4x4, BOARD_SIZE_4x4)\n",
    "\n",
    "    # Train and plot for 5x5 board\n",
    "    agent_5x5 = DeepQLearningAgent(BOARD_SIZE_5x5)\n",
    "    win_rates_5x5 = agent_5x5.train(1000)\n",
    "    plot_win_rate(win_rates_5x5, BOARD_SIZE_5x5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
